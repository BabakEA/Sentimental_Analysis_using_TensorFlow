{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install -q tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup # remove the html tags like <br> <tr> ....\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, preprocessing, models, layers,regularizers\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB Dataset : Inclouding reviews and emotions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds # Tensorflow datasets \n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True) # call the IMDB dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Function to Remove Html Tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_special_chars(st):\n",
    "    st = BeautifulSoup(st, \"lxml\").text\n",
    "    my_pattern = '[A-Za-z0-9.! ]+'\n",
    "    return ''.join(re.findall(my_pattern, st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Test and Train Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for s,l in train_data:#get sentences and correspondent labels from the tensor dataset\n",
    "    training_sentences.append(strip_special_chars(str(s.numpy()).lower()))\n",
    "    #training_sentences.append(str(s.numpy()).lower())\n",
    "    training_labels.append(l.numpy())\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(strip_special_chars(str(s.numpy()).lower()))\n",
    "    #testing_sentences.append(str(s.numpy()).lower())\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the review is :  Negative\n",
      "bthis is a big step down after the surprisingly enjoyable original. this sequel isnt nearly as fun as part one and it instead spends too much time on plot development. tim thomerson is still the best thing about this series but his wisecracking is toned down in this entry. the performances are all adequate but this time the script lets us down. the action is merely routine and the plot is only mildly interesting so i need lots of silly laughs in order to stay entertained during a trancers movie. unfortunately the laughs are few and far between and so this film is watchable at best. \n",
      "\n",
      "the review is :  Negative\n",
      "bperhaps because i was so young innocent and brainwashed when i saw it this movie was the cause of many sleepless nights for me. i havent seen it since i was in seventh grade at a presbyterian school so i am not sure what effect it would have on me now. however i will say that it left an impression on me... and most of my friends. it did serve its purpose at least until we were old enough and knowledgeable enough to analyze and create our own opinions. i was particularly terrified of what the newlyconverted postrapture christians had to endure when not receiving the mark of the beast. i dont want to spoil the movie for those who havent seen it so i will not mention details of the scenes but i can still picture them in my head... and its been 19 years. \n",
      "\n",
      "the review is :  Positive\n",
      "bhood of the living dead had a lot to live up to even before the opening credits began. first any play on ...of the living dead invokes his holiness mr. romero and instantly sets up a high standard to which many movies cannot afford to aspire. and second my moviewatching companion professed doubt that any urban horror film would surpass the seminal leprechaun in the hood. skeptical we settled in to watch. we were rewarded with a surprisingly sincere and goodhearted zombie film. oh certainly the budget is low and of course the directors amateurs friends populate the cast but hood of the living dead loves zombie cinema. cheap yeah. but when its this cheap you can clearly see where love holds it together. ricky works in a lab during the day and as a surrogate parent to his younger brother at night. he dreams of moving out of oakland. before this planned escape however his brother is shot to death in a driveby. rickys keen scientific mind presents an option superior to cpr or 911 injections of his labs experimental regenerative formula. sadly little bro wakes up in an ambulance as a bloodthirsty oakland zombie! chaos and mayhem! i think its more economical to eat your enemies than take vengeance in a driveby but then again im a poor judge of the complexities of urban life. how poor a judge in response to a gory scene involving four men i opined ahha! white tshirts on everyone so the blood shows up. economical! i used the same technique in my own lowbudget horror film. jordan replied no thats gang dress. white tshirts were banned from new orleans bars for a time as a result. oh.a lot of the movie is set in someones living room so theres a great deal of hanging out and waiting for the zombies. but the characters are sympathetic and the movie is sincere it surpasses its budget in spirit. zombie explanation when man plays god zombies arise! or perhaps follow fdaapproved testing rules before human experimentation! contribution to the zombie canon this is the first zombie movie ive seen with a driveby shooting. as far as the actual zombies go infection is spread with a bite as usual but quite unusually head shots dont work its heart shots that kill. zombies have pulses the absence of which proves true death. and these zombies make pretty cool jaguargrowl noises. gratuitous zombie movie injoke a mercenary named romero. groan. favorite zombie jaguarnoise little brother zombie of course! \n",
      "\n",
      "the review is :  Negative\n",
      "bfor me this is a story that starts with some funny jokes regarding franks fanatasies when he is travelling with a staircase and when he is sitting in business meetings... the problem is that when you have been watching this movie for an hour you will see the same fantasiesfunny situations again and again and again. it is to predictable. it is more done as a tv story where you can go away and come back without missing anything.i like felix herngren as frank but that is not enough even when it is a comedy it has to have more variations and some kind of message to its audience.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### test a dataset shape and sentences\n",
    "\n",
    "Dict={0:\"Negative\",1:\"Positive\"}\n",
    "\n",
    "\n",
    "for i in range (4):\n",
    "    \n",
    "    print(\"the review is : \",Dict[training_labels_final[i]])\n",
    "    print(training_sentences[i],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer function \n",
    "### Create the dictionary of known words as your bag of knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000 # number of knowing Words \n",
    "embedding_dim = 16 # similarities dimension for word embedding function \n",
    "max_length = 200 # sentences length to build the model\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\" # codding for unknowing words\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# creat tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False,document_count=0)\n",
    "\n",
    "\n",
    "# train the tokenizer #### it's required for the prodoction level as well \n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "#creat the training set : \n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "#padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "padded = pad_sequences(sequences,maxlen=max_length,padding=\"post\")\n",
    "\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding=\"post\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save the Tokenizer and padding Functions \n",
    "\n",
    "pickle.dump(tokenizer, open( \"tokenizer.pkl\", \"wb\" ) )\n",
    "#pickle.dump( tokenizer, open( \"tokenizer.pkl\", \"wb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisanewword the problem is that when you have been watching this movie for an hour you will see the same fantasiesfunny situations again and again and again \n",
      " *************** \n",
      " [[1, 2, 433, 7, 12, 52, 22, 25, 74, 147, 10, 17, 15, 33, 561, 22, 76, 65, 2, 166, 1, 1153, 170, 3, 170, 3, 170]] \n",
      " **************** \n",
      " [[   1    2  433    7   12   52   22   25   74  147   10   17   15   33\n",
      "   561   22   76   65    2  166    1 1153  170    3  170    3  170    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "test=\"\"\"thisanewword the problem is that when you have been watching this movie for an hour you will see the same fantasiesfunny situations again and again and again\"\"\"\n",
    "\n",
    "test_token=tokenizer.texts_to_sequences([test])\n",
    "test_paded=pad_sequences(test_token,maxlen=max_length,padding=\"post\")\n",
    "\n",
    "print(test ,\"\\n\",\"*************** \\n\",test_token ,\"\\n\",\"**************** \\n\",test_paded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.TruePositives()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.95):\n",
    "            print(\"\\nReached 95.% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regularizers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-506492342c56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regularizers' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    layers.SeparableConv1D(filters=10, kernel_size=5, strides=3, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, kernel_regularizer=regularizers.l1(0.001),activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.TruePositives()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.6329 - val_accuracy: 0.8286\n",
      "\n",
      "Reached 95.% accuracy so cancelling training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14db31d4ac8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final),callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Sequential.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test a New Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_convertore(text,max_length=200):\n",
    "    text=strip_special_chars(text.lower())\n",
    "    test=tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    paded_test=pad_sequences(test,maxlen=max_length,padding=\"post\")\n",
    "    \n",
    "    return paded_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   9   43   46    1  516   18    8    2  126  326   23 3899    6   72\n",
      "   429   31  402    3  108  498    6  731   42    9    8    4 2266 2734\n",
      "     7   84 1223    6  123  520   12   46    5    2 1979  602   20    2\n",
      "   119    7   21  711    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "[[0.00458202]]\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "It has some iffy parts but in the end kids are exposed to much worse at school and being able to talk about it in a safe environment is great. Explain to your child that some of the behavior shown on the show is not OK.\n",
    "\"\"\"\n",
    "test=text_convertore(text,max_length=200)\n",
    "print(test)\n",
    "\n",
    "print(model.predict(test))\n",
    "print(Dict[model.predict_classes(test)[0][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   9   13   28    5    2  113  179   12  121  559    8   57  117    3\n",
      "    11   41 1742    9   37   11 3394 1285   30    2  174    3 1139   15\n",
      "   248  142  136   84  374   11  114  458    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "[[0.9742953]]\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "Test_1=\"\"\"\n",
    "it was one of the best things that ever happened in my life and i just finished it so i wanna thank all the cast and producers for making those such great moments.i love guys.\n",
    "\n",
    "\"\"\"\n",
    "Test_1=text_convertore(Test_1,max_length=200)\n",
    "print(Test_1)\n",
    "\n",
    "print(model.predict(Test_1))\n",
    "print(Dict[model.predict_classes(Test_1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20093167]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20093167]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
